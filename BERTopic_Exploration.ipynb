{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Required Libraries"
      ],
      "metadata": {
        "id": "5_uYy0BwRvx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install umap\n",
        "!pip install umap-learn\n",
        "!pip install nltk\n",
        "!pip install sentence_transformers\n",
        "!pip install HDBSCAN"
      ],
      "metadata": {
        "id": "i7GH4MBKSg_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"textminr/cmu-book-summaries\")['train']\n",
        "summaries = dataset['summary']"
      ],
      "metadata": {
        "id": "NUuYijcPRoga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract canonical authors only:"
      ],
      "metadata": {
        "id": "8yTnAiV_R0nR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "target_authors = [\n",
        "    'Louisa May Alcott',\n",
        "    'Ray Bradbury',\n",
        "    'Willa Cather',\n",
        "    'Louise Erdrich',\n",
        "    'Paul Fleischman',\n",
        "    'Russell Freedman',\n",
        "    'Laura Hillenbrand',\n",
        "    'Sue Monk Kidd',\n",
        "    'Robert Louis Stevenson',\n",
        "    'Mark Twain',\n",
        "    'Gloria Whelan',\n",
        "    'Elie Wiesel',\n",
        "    'Sarah Hopkins Bradford',\n",
        "    'Emily Bronte',\n",
        "    'Agatha Christie',\n",
        "    'Charles Dickens',\n",
        "    'F. Scott Fitzgerald',\n",
        "    'William Golding',\n",
        "    'Harper Lee',\n",
        "    'Madeleine L’Engle',\n",
        "    'Arthur Miller',\n",
        "    'Bill O’Reilly',\n",
        "    'George Orwell',\n",
        "    'Jack Schaefer',\n",
        "    'William Shakespeare',\n",
        "    'John Steinbeck',\n",
        "    'Sabaa Tahir',\n",
        "    'Jane Austen',\n",
        "    'Nathaniel Hawthorne',\n",
        "    'Ernest Hemingway',\n",
        "    'J.D. Salinger',\n",
        "    'Miguel de Cervantes',\n",
        "    'Mary Shelley',\n",
        "    'Geoffrey Chaucer',\n",
        "    'Leo Tolstoy',\n",
        "    'Albert Camus',\n",
        "    'John Milton',\n",
        "    'Homer',\n",
        "    'Geoffrey Chaucer',\n",
        "    'Miguel de Cervantes',\n",
        "    'William Shakespeare',\n",
        "    'William Wordsworth',\n",
        "    'Charles Dickens',\n",
        "    'Jane Austen',\n",
        "    'Mark Twain',\n",
        "    'Leo Tolstoy',\n",
        "    'Harper Lee',\n",
        "    'F. Scott Fitzgerald',\n",
        "    'Nathaniel Hawthorne',\n",
        "    'Ernest Hemingway',\n",
        "    'George Orwell',\n",
        "    'J.D. Salinger',\n",
        "    'William Faulkner',\n",
        "    'John Steinbeck',\n",
        "    'Zora Neale Hurston',\n",
        "    'Maya Angelou',\n",
        "    'Fyodor Dostoevsky',\n",
        "    'Geoffrey Chaucer',\n",
        "    'John Steinbeck',\n",
        "    'Mary Shelley',\n",
        "    'S.E. Hinton',\n",
        "    'John Milton',\n",
        "    'Albert Camus',\n",
        "    'Louisa May Alcott',\n",
        "    \"Bill O'Reilly\",\n",
        "    'Ray Bradbury',\n",
        "    'Arthur Miller',\n",
        "    'Edgar Allan Poe'\n",
        "\n",
        "]\n",
        "\n",
        "filtered_df_canon = df[df['author'].isin(target_authors)]\n"
      ],
      "metadata": {
        "id": "2kVtt_NWRzeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize by sentence and remove phrases which are meaningless, but become too representative of topics."
      ],
      "metadata": {
        "id": "TvOoYf8J_hsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize_summaries_into_sentences(summaries, phrases_to_replace):\n",
        "    tokenized_sentences = []\n",
        "    for summary in summaries:\n",
        "        sentences = sent_tokenize(summary)\n",
        "        for sentence in sentences:\n",
        "            for phrase in phrases_to_replace:\n",
        "                sentence = sentence.replace(phrase, \"it\")\n",
        "            tokenized_sentences.append(sentence)\n",
        "    return tokenized_sentences\n",
        "\n",
        "phrases_to_replace = [\"this novel\", \"novel\", \"this book\", \"characters\", \"the story\", \"this story\", \"The story\", \"the novel\", \"narrator\", \"book\", \"chapter\"]\n",
        "\n",
        "\n",
        "tokenized_sentences = tokenize_summaries_into_sentences(summaries, phrases_to_replace)\n"
      ],
      "metadata": {
        "id": "SYJJTtjjTAUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3cce380-8dc6-466c-de95-b2d1f6e6811e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embed the Sentences"
      ],
      "metadata": {
        "id": "omrNe94OUh2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer(\"all-distilroberta-v1\")\n",
        "\n",
        "embeddings = embedding_model.encode(tokenized_sentences, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "JWeZKwlgUjrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality Reduction"
      ],
      "metadata": {
        "id": "ERD5gz5rU6iP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from umap import UMAP\n",
        "\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)"
      ],
      "metadata": {
        "id": "p8nObdeKU5nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering"
      ],
      "metadata": {
        "id": "4LTDn-nLTS1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hdbscan import HDBSCAN\n",
        "\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=100, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
      ],
      "metadata": {
        "id": "-FVFsOt2TH5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorizer"
      ],
      "metadata": {
        "id": "hEaEv7RGUy4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=5, ngram_range=(1, 4))"
      ],
      "metadata": {
        "id": "Cwlt4NhSUYeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize GPT for Interpretable Topics"
      ],
      "metadata": {
        "id": "Rc0koEW__KeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, OpenAI, PartOfSpeech\n",
        "\n",
        "\n",
        "# GPT-3.5\n",
        "prompt = \"\"\"\n",
        "'I have a topic that contains the following documents: [DOCUMENTS]\n",
        "The topic is described by the following keywords: [KEYWORDS]\n",
        "\n",
        "Based on the information above, extract a short topic label of at most 3 words. It should be a theme.\n",
        "topic: <theme>\n",
        "\"\"\"\n",
        "client = openai.OpenAI(api_key=\"sk-********************\") # Requires API Key\n",
        "openai_model = OpenAI(client, model=\"gpt-3.5-turbo\", exponential_backoff=True, chat=True, prompt=prompt)\n",
        "\n",
        "representation_model = {\n",
        "    \"OpenAI\": openai_model # If you don't have API, just comment this out.\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "R-YazFCf_MND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and Fit the Model"
      ],
      "metadata": {
        "id": "W9yciVtAoHdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "topic_model = BERTopic(\n",
        "\n",
        "  # Pipeline models\n",
        "  embedding_model=embedding_model,\n",
        "  umap_model=umap_model,\n",
        "  hdbscan_model=hdbscan_model,\n",
        "  vectorizer_model=vectorizer_model,\n",
        "\n",
        "  # Hyperparameters\n",
        "  top_n_words=10,\n",
        "  verbose=True\n",
        "  calculate_probabilities=True)\n",
        "\n",
        ")\n",
        "topics, probs = topic_model.fit_transform(tokenized_sentences, embeddings)"
      ],
      "metadata": {
        "id": "j5UkLIHcVd2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Topic List"
      ],
      "metadata": {
        "id": "xAdO5HG6-2XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "WDszvIy5-3XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine Similarity"
      ],
      "metadata": {
        "id": "c9TkyF7_81lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_heatmap()"
      ],
      "metadata": {
        "id": "OPedNko58uH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outlier Reduction and Visualization"
      ],
      "metadata": {
        "id": "lWRSeKZC8561"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.reduce_outliers()\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "topic_counts_before = Counter(topics)\n",
        "topic_counts_after = Counter(new_topics)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(topic_counts_before.keys(), topic_counts_before.values(), alpha=0.5, label='Before Reduction')\n",
        "plt.bar(topic_counts_after.keys(), topic_counts_after.values(), alpha=0.5, label='After Reduction')\n",
        "plt.xlabel('Topic ID')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Topic Distribution Before and After Outlier Reduction')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "s2EHgO-A80JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perplexity:"
      ],
      "metadata": {
        "id": "i9XAZSG-oF-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_probabilities=True)\n",
        "\n",
        "topics, probs = model.fit_transform(tokenized_sentences) # docs = dataset\n",
        "log_perplexity = -1 * np.mean(np.log(np.sum(probs, axis=1)))\n",
        "perplexity = np.exp(log_perplexity)"
      ],
      "metadata": {
        "id": "tc68skj8oEO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Aspect Representation Modeling with P.O.S. Just replace this in the initialization cell."
      ],
      "metadata": {
        "id": "n8jysR0R_pzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, OpenAI, PartOfSpeech\n",
        "\n",
        "\n",
        "# GPT-3.5\n",
        "prompt = \"\"\"\n",
        "'I have a topic that contains the following documents: [DOCUMENTS]\n",
        "The topic is described by the following keywords: [KEYWORDS]\n",
        "\n",
        "Based on the information above, extract a short topic label of at most 3 words. It should be a theme.\n",
        "topic: <theme>\n",
        "\"\"\"\n",
        "client = openai.OpenAI(api_key=\"sk-********************\") # Requires API Key\n",
        "openai_model = OpenAI(client, model=\"gpt-3.5-turbo\", exponential_backoff=True, chat=True, prompt=prompt)\n",
        "\n",
        "representation_model = {\n",
        "    \"OpenAI\": openai_model, # If you don't have API, just comment this out.\n",
        "    \"POS\": pos_model\n",
        "}"
      ],
      "metadata": {
        "id": "ShPOrxRW_sey"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}